![header](https://capsule-render.vercel.app/api?type=soft&color=auto&height=300&section=header&text=vaccine%20Review💉&fontSize=90)

# 🦠 COVID Vaccine Controversy Analysis by BERT
**Multilingual BERT를 활용한 코로나 백신 여론 분석 프로젝트**

[![PyTorch](https://img.shields.io/badge/PyTorch-E34F26?style=flat-square&logo=PyTorch&logoColor=white)](https://pytorch.org/)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-FFD21C?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=Python&logoColor=white)](https://www.python.org/)

---

##  1. 개요 및 목표 (Overview & Thesis)

###  문제 정의 및 프로젝트의 의의
본 프로젝트는 COVID-19 팬데믹이 지나간 현재 시점에서,  
그동안 온라인 커뮤니티에 남겨진 댓글과 리뷰를 수집을 한 후 필요없는 데이터나 가비지 데이터를 정제하여 유의미한 정보만을 남긱고 그것을 분석해 보는 것을 목표로 합니다.


- **핵심 목표**: 약 **11만 건의 리뷰 데이터**를 분석해  
  사회적 의무(Mandate) 논란이 **과학적 부작용보다 여론 확산에 더 큰 영향을 미쳤음**을 입증  
- **성공 지표**: 2만 건의 수동 라벨링 데이터 기반 **F1 Score = 0.6438** 달성 → 모델 신뢰성 확보

---

##  2. 데이터 수집 및 정제 과정 (극복의 스토리)

### 2-1. 데이터 확보 난관 및 최종 소스

> 본 프로젝트는 Naver, WebMD, Drugs.com 등 **6가지 크롤링 및 API 시도**를 거쳐  
> **Reddit API**를 중심으로 **98,277건의 고품질 데이터**를 확보했습니다.

| 소스 | 언어 | 수집 내용 | 결과 / 문제점 |
|------|------|-----------|----------------|
| **Naver 지식iN** | 🇰🇷 | Q&A (타이레놀/피임약) | 답변 32,000건 확보. 단, 백신 관련 내용 부족 및 질문 본문 확인 불가 |
| **Naver News 댓글** | 🇰🇷 | 뉴스 댓글 감성 분석 | 실패: Selenium CSS 선택자 불일치 |
| **DC Inside** | 🇰🇷 | 포럼 댓글 감성 분석 | 23,000건 확보. 비속어 및 백신 무관한 내용 다수 |
| **Reddit API (PRAW)** | 🇺🇸 | 댓글/게시글 (Top Posts) | ✅ 72,827건 확보 (핵심 데이터) |
| **Pushshift API** | 🇺🇸 | 과거 Reddit 데이터 | ❌ 403 Forbidden (서버 차단), 약 2,100건 확보 |
| **Drugs.com** | 🇺🇸 | 전문 리뷰 | ❌ 403 IP 차단, 약 1500건 확보 |
| **WebMD** | 🇺🇸 | 포럼 댓글 | ❌ 403 IP 차단, 약 16,800건 확보 |
| **HealthBoards** | 🇺🇸 | 포럼 댓글 | ❌ 404 오류, 약 3,100건 확보 |
| **Patient.info** | 🇺🇸 | 리뷰/포럼 | ❌ Requests 차단, 약 480건 확보 |

**최종 데이터 통합**
-  Reddit + WebMD + Pushshift + HealthBoards + Patient.info + Drugs.com 통합
-  총 **약 12만 건** (영문 및 한글 포함)
-  이후 **한국어 데이터 제거 → 영어 데이터만 남김**

-  최종 데이터: 약 10만8천건 (영문만 있음)

---

##  3. 데이터 전처리 파이프라인 (4단계)

###  단계 1: 구조적 노이즈 제거 (API Placeholder)

| 가비지 유형 | 문제 원인 | 적용 기법 | 처리 내용 |
|:-------------|:------------|:------------|:------------|
| `[deleted]`, `[No Content]` | Reddit API 삭제 게시물 | `pandas filtering` | 해당 문자열 포함 행 제거 |
| 짧은 잡담 (`lol`, `ok`) | 의미 없는 짧은 댓글 | `length check` | 20자 미만 / 5단어 미만 제거 |

---

### 🔹 단계 2: 언어적 노이즈 제거 (한국어 분리)

| 가비지 유형 | 문제 원인 | 적용 기법 | 처리 내용 |
|:-------------|:------------|:------------|:------------|
| 한글 포함 데이터 | Naver 크롤링 데이터 | `RegEx filtering` | 한글 비율 10% 초과 시 제거 |

---

###  단계 3: 형식적 노이즈 제거 (특수문자, URL 등)

| 가비지 유형 | 문제 원인 | 적용 기법 | 처리 내용 |
|:-------------|:------------|:------------|:------------|
| 특수문자/URL | 이모티콘, 반복 기호 | `RegEx ratio check` | 알파벳 외 문자 비율 40% 초과 제거 |
| 불용어 | the, a, is, covid, vaccine 등 | `Stopword list` | 핵심 키워드(`mask`, `mandate`) 중심으로 토픽 모델링 효율 개선 |

---

##  4. 토픽 모델링 (Topic Modeling)

### 코드 요약

| 단계 | 주요 내용 | 사용 라이브러리 / 함수 | 목적 |
|------|------------|--------------------------|------|
| 1️⃣ 데이터 로드 | `Real_Final.csv` 불러오기 | `pandas.read_csv()` | 데이터 준비 |
| 2️⃣ 전처리 | URL, 특수문자 제거 + 토큰화 + 표제어화 | `re`, `nltk` | 노이즈 제거 |
| 3️⃣ DTM 생성 | BoW 변환 및 희귀 단어 제거 | `gensim.Dictionary`, `doc2bow()` | 텍스트 수치화 |
| 4️⃣ LDA 학습 | `num_topics=10`, `passes=20` | `gensim.models.LdaModel` | 주요 주제 추출 |
| 5️⃣ 결과 해석 | 각 토픽 상위 10개 단어 분석 | `lda_model.print_topics()` | 핵심 주제 파악 |

---
이 과정들을 통해 이전 데이터: 약 10만8천건  ---> 9만 7천건까지 정제하였다.

###  LDA 토픽 모델링 결과 (K=10)

| 토픽 번호 | 주요 키워드 | 해석된 주제 | 핵심 논의 내용 |
|------------|--------------|--------------|----------------|
| **1** | state, government, school, free, business | 국가 정책 및 지역 행정 | 정부 정책, 학교 운영, 사업 규제 등 |
| **2** | people, doctor, health, risk, vaccination | 일반인 건강 및 의료 접근성 | 개인 건강, 접종 필요성, 위험 인식 |
| **3** | company, money, debt, share, loan | 경제적 영향 및 기업 금융 | 코로나가 금융·기업에 미친 영향 |
| **4** | like, dont, think, time, year | 개인의 생각 및 감정 표현 | 일상적 감정, 의견 공유 중심 |
| **5** | mask, wear, face, protect, approved | 마스크 착용 및 방역 조치 | 마스크 의무화, 보호 장비 논의 |
| **6** | side, country, trump, american, world | 정치적 갈등 및 국가 상황 | 미국 중심의 정치·사회적 갈등 |
| **7** | insurance, cost, headache, paid, market | 비용 및 보험 문제 | 의료비, 보험, 시장 변동성 |
| **8** | vaccine, covid, effect, virus, booster | 백신 효능 및 과학적 논의 | 백신 효과, 바이러스, 부스터샷 |
| **9** | week, work, month, sick, day | 일상 및 근무 환경 변화 | 재택근무, 격리 등 생활 패턴 변화 |
| **10** | post, read, comment, source, link | 정보 공유 및 커뮤니티 소통 | 게시글·링크를 통한 정보 교환 |

---

###  종합 시사점
- **토픽 3 & 7**: 경제적 영향(기업 vs. 개인 비용)이 주요 논의 축  
- **토픽 5 & 8**: 방역 조치 및 백신 효능 관련 과학적 논의  
- **토픽 6**: 정치적 분열과 국가적 감정이 백신 담론에 영향  
- **토픽 10**: 신뢰할 수 있는 정보 출처에 대한 사회적 갈망 반영  

---

##  5. 단어 빈도 분석 (Word Frequency Analysis)

### 코드 요약

| 단계 | 주요 내용 | 사용 라이브러리 | 목적 |
|------|------------|------------------|------|
| 1️⃣ 데이터 로드 및 전처리 | 불용어 제거, 표제어 추출 | `pandas`, `re`, `nltk` | 분석 정확도 향상 |
| 2️⃣ 단어 빈도 계산 | 전체 문서에서 단어 집계 | `collections.Counter` | 주요 단어 정량 분석 |
| 3️⃣ 상위 50개 키워드 추출 | `most_common()` 사용 | `Counter` | 핵심 관심사 파악 |

> 🔹 **LDA vs. 단어 빈도 비교**
> - LDA: 단어 간 *연관성* 기반 주제 도출  
> - 단어 빈도: 단순 *언급 횟수* 기반 주요 키워드 파악  
>  
> 두 결과를 교차 검증함으로써 분석의 신뢰도를 강화했습니다.

---

###  상위 50개 단어 (빈도 기준)

| 순위 | 단어 | 빈도 | 순위 | 단어 | 빈도 |
|------|------|------|------|------|------|
| 1 | people | 26,363 | 26 | youre | 4,970 |
| 2 | like | 14,459 | 27 | much | 4,961 |
| 3 | dont | 13,564 | 28 | want | 4,946 |
| 4 | vaccine | 11,350 | 29 | well | 4,879 |
| 5 | would | 9,974 | 30 | virus | 4,876 |
| 6 | mask | 9,539 | 31 | could | 4,791 |
| 7 | covid | 8,198 | 32 | many | 4,589 |
| 8 | know | 8,058 | 33 | case | 4,079 |
| 9 | time | 8,029 | 34 | country | 4,041 |
| 10 | think | 7,550 | 35 | doesnt | 4,040 |
| 11 | even | 7,454 | 36 | week | 3,998 |
| 12 | thing | 6,749 | 37 | getting | 3,931 |
| 13 | thats | 6,710 | 38 | back | 3,792 |
| 14 | work | 6,704 | 39 | life | 3,765 |
| 15 | make | 6,564 | 40 | mean | 3,687 |
| 16 | need | 6,468 | 41 | didnt | 3,682 |
| 17 | going | 6,355 | 42 | state | 3,660 |
| 18 | still | 6,263 | 43 | everyone | 3,654 |
| 19 | also | 6,230 | 44 | someone | 3,643 |
| 20 | right | 6,181 | 45 | home | 3,607 |
| 21 | year | 5,981 | 46 | said | 3,603 |
| 22 | good | 5,393 | 47 | every | 3,466 |
| 23 | really | 5,264 | 48 | month | 3,447 |
| 24 | cant | 5,049 | 49 | point | 3,413 |
| 25 | take | 5,020 | 50 | first | 3,396 |

---

## 결론 (Conclusion)
- **사회적 논란의 핵심 축**은 과학적 부작용보다 **정치·경제·사회적 이슈**에 집중  
- **LDA와 빈도 분석** 결과가 서로 일치하며, **마스크·백신·정부정책·경제영향**이 공통 핵심 키워드로 등장  
- 이는 코로나 백신 논란이 단순한 의학적 문제를 넘어, **사회적 신뢰와 정책적 갈등의 문제**였음을 시사




이후, 전처리 전 데이터에서 떠오르는 토픽들이 코로나나 백신과 너무 관련이 없다는 것을 확인하고 키워드를 골라서 그 키워드가 있는 댓글들은 True/ 없는 경우를 false 분류하고 false 댓들들은 삭제하는 전처리를 진행했다.

 ## 🔍 주제 관련성 키워드 분류 코드 요약

| 구분 | 주요 내용 | 사용 라이브러리 / 함수 | 목적 및 중요성 |
|------|------------|--------------------------|----------------|
| **1단계: 데이터 준비** | `Real_Final.csv` 파일을 로드하고, 결측값을 제거하여 데이터의 무결성을 확보합니다. | `pandas.read_csv()`, `dropna()` | 분석 대상 텍스트 데이터를 정제하고, 초기 데이터 건수를 확인하여 분석의 기준점을 설정합니다. |
| **2단계: 주제 정의 및 확장** | COVID-19 및 백신 관련 핵심 키워드 20여 개를 명시적으로 정의합니다.<br>(예: *vaccine, covid, side effect, mask, booster, mrna* 등) | `Python list` | 논란 분석에 필요한 핵심 주제의 범위를 구체적으로 한정합니다. 정의된 키워드가 포함된 텍스트만 분석 대상으로 삼아 효율성을 높입니다. |
| **3단계: 이진 분류 로직** | 각 텍스트를 소문자화한 후, 정의된 키워드 중 하나라도 포함되어 있는지 여부를 확인하여 **True(관련 있음)** 또는 **False(관련 없음)**으로 분류합니다. | `str.lower()`, `any()` | 데이터셋을 **‘주제 관련 데이터’**와 **‘주제 무관 데이터’**로 구분하여 필터링 기준을 설정합니다. 이후 논란 분석 등 핵심 분석에 집중할 수 있는 기반을 마련합니다. |
| **4단계: 결과 분석 및 저장** | 분류된 결과를 기반으로 관련 데이터의 건수와 비율을 산출하고, 최종적으로 `is_related_topic` 컬럼이 포함된 새로운 CSV(`FINAL_DATA_CLEANED_CLASSIFIED_V2.csv`)로 저장합니다. | `df.to_csv()` | 분류된 데이터의 통계적 분포를 확인하고, **후속 심층 분석(LDA·감성 분석 등)**의 기반 데이터를 확정합니다. |

---

KEYWORDS = [
    'vaccine', 'covid', 'coronavirus', 'side effect', 'adverse', 'pfizer', 'moderna',
    'booster', 'jab', 'shot', 'vax', 'myocarditis', 'astrazeneca', 'janssen',
    'symptoms', 'mandate', 'mask', 'masked', 'unvaccinated', 'vaxxed', 'unvaxxed',
    'hospital', 'death', 'long covid', 'long-covid', 'spike protein', 'mrna' ]
 이렇게 잡았다.



### 💡 보고서 활용 및 분석적 시사점

1. **분석 범위의 명확화**  
   전체 데이터(약 98,277건) 중에서 **핵심 키워드 기반 필터링**을 통해  
   COVID-19 및 백신 관련 텍스트만 선별함으로써,  
   이후 분석(예: 토픽 모델링, 감성 분석)이 **핵심 논의 중심 데이터에 집중**되도록 함.  
   > **보고서 예시 문구:**  
   > “전체 데이터 중 약 **XX%**가 COVID-19/백신 관련 키워드 기반으로 분류되어 후속 분석에 활용되었다.”

2. **키워드 기반 분류의 한계와 보완 (Feat. LDA)**  
   - 키워드 미포함 관련 텍스트 누락, 키워드 포함 비관련 텍스트 포함 등 **오분류 가능성** 존재.  
   - 하지만, **LDA 토픽 모델링 결과를 바탕으로 핵심 키워드를 확장 정의**하여  
     이 단계를 **전략적 필터링 과정**으로 활용할 수 있음.  
     즉, “LDA로 주제 후보를 도출 → 본 코드로 해당 주제를 대표하는 텍스트 선별”의 **2단계 분석 체계**로 설명 가능.


 그 결과 --- 필터링 결과 ---
✅ True (관련 있음) 데이터만 남김: 22939건
❌ False (무관함) 데이터 제거됨: 75338건

